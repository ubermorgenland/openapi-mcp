# Memory Optimization Architecture

The OpenAPI MCP system implements comprehensive memory optimization strategies to handle large API specifications and high-throughput operations without memory exhaustion.

## Memory Management Overview

```mermaid
graph TB
    subgraph "Memory Management System"
        subgraph "Pool Management"
            BytePool[Byte Pool]
            BufferPool[Buffer Pool]
            StreamProcessor[Stream Processor]
        end
        
        subgraph "Memory Monitoring"
            MemLimiter[Memory Limiter]
            GCTrigger[GC Trigger]
            MemStats[Memory Statistics]
        end
        
        subgraph "Processing Strategies"
            Streaming[Streaming Processing]
            Chunking[Chunked Processing]
            Batching[Batch Processing]
        end
        
        subgraph "Optimization Targets"
            LargeSpecs[Large OpenAPI Specs]
            HighThroughput[High Throughput APIs]
            LongRunning[Long Running Operations]
        end
    end
    
    BytePool --> StreamProcessor
    BufferPool --> StreamProcessor
    StreamProcessor --> Streaming
    
    MemLimiter --> GCTrigger
    MemStats --> MemLimiter
    
    Streaming --> LargeSpecs
    Chunking --> HighThroughput
    Batching --> LongRunning
```

## Core Memory Components

### 1. Buffer Pool System
**Location**: `pkg/memory/pool.go`

The buffer pool system provides reusable memory buffers to eliminate allocation overhead:

```mermaid
graph LR
    subgraph "Buffer Pool Lifecycle"
        Request[Request Arrives]
        GetBuffer[Get Buffer from Pool]
        UseBuffer[Use Buffer for Processing]
        ReturnBuffer[Return Buffer to Pool]
        Reuse[Buffer Available for Reuse]
        
        Request --> GetBuffer
        GetBuffer --> UseBuffer
        UseBuffer --> ReturnBuffer
        ReturnBuffer --> Reuse
        Reuse --> GetBuffer
    end
    
    subgraph "Pool Types"
        BytePool[Byte Slice Pool]
        BufferPool[bytes.Buffer Pool]
    end
    
    GetBuffer -.-> BytePool
    GetBuffer -.-> BufferPool
```

#### Byte Pool Implementation
```go
type BytePool struct {
    pool sync.Pool
}

func NewBytePool(initialSize int) *BytePool {
    return &BytePool{
        pool: sync.Pool{
            New: func() interface{} {
                return make([]byte, 0, initialSize)
            },
        },
    }
}

func (bp *BytePool) Get() []byte {
    return bp.pool.Get().([]byte)[:0] // Reset length but keep capacity
}

func (bp *BytePool) Put(b []byte) {
    // Only return large enough slices to avoid memory fragmentation
    if cap(b) >= 1024 {
        bp.pool.Put(b)
    }
}
```

### 2. Memory Limiter
**Location**: `pkg/memory/pool.go`

Monitors and controls memory usage to prevent out-of-memory conditions:

```mermaid
graph TB
    subgraph "Memory Limiter Flow"
        Operation[Operation Start]
        Check[Check Memory Usage]
        Decision{Memory OK?}
        Continue[Continue Operation]
        TriggerGC[Trigger Garbage Collection]
        Recheck[Recheck Memory]
        Success{Below Limit?}
        Fail[Operation Failed]
        
        Operation --> Check
        Check --> Decision
        Decision -->|Yes| Continue
        Decision -->|No| TriggerGC
        TriggerGC --> Recheck
        Recheck --> Success
        Success -->|Yes| Continue
        Success -->|No| Fail
    end
```

#### Memory Limiter Implementation
```go
type MemoryLimiter struct {
    maxMemoryMB    int64
    checkInterval  int
    operationCount int64
    mu             sync.Mutex
}

func (ml *MemoryLimiter) CheckMemoryUsage() bool {
    ml.mu.Lock()
    defer ml.mu.Unlock()
    
    ml.operationCount++
    
    // Only check memory every N operations to avoid overhead
    if ml.operationCount%int64(ml.checkInterval) != 0 {
        return true
    }
    
    var m runtime.MemStats
    runtime.ReadMemStats(&m)
    
    currentMemoryMB := int64(m.Alloc) / (1024 * 1024)
    
    if currentMemoryMB > ml.maxMemoryMB {
        runtime.GC() // Force garbage collection
        
        // Check again after GC
        runtime.ReadMemStats(&m)
        currentMemoryMB = int64(m.Alloc) / (1024 * 1024)
        
        return currentMemoryMB <= ml.maxMemoryMB
    }
    
    return true
}
```

## Streaming Processing Architecture

### 1. Large JSON Processing
**Location**: `pkg/memory/streaming.go`

Handles large JSON responses without loading everything into memory:

```mermaid
sequenceDiagram
    participant Client
    participant Processor as JSON Processor
    participant Decoder as JSON Decoder
    participant MemLimiter as Memory Limiter
    participant Callback as Processing Callback
    
    Client->>Processor: Large JSON Stream
    Processor->>Decoder: Create streaming decoder
    
    loop For each JSON object
        Decoder->>MemLimiter: Check memory usage
        MemLimiter-->>Decoder: Memory OK
        Decoder->>Decoder: Decode single object
        Decoder->>Callback: Process object
        Callback-->>Decoder: Processing complete
        Note over Decoder: Object eligible for GC
    end
    
    Processor-->>Client: Processing complete
```

#### Streaming JSON Implementation
```go
func (sjp *StreamingJSONProcessor) ProcessLargeJSON(ctx context.Context, reader io.Reader, callback func(interface{}) error) error {
    decoder := json.NewDecoder(reader)
    decoder.UseNumber() // Handle large numbers safely
    
    var processedCount int
    
    for {
        // Check context cancellation
        select {
        case <-ctx.Done():
            return ctx.Err()
        default:
        }
        
        // Check memory usage periodically
        if processedCount%100 == 0 && !sjp.processor.CheckMemory() {
            return fmt.Errorf("memory usage exceeded limits")
        }
        
        var item interface{}
        if err := decoder.Decode(&item); err != nil {
            if err == io.EOF {
                break // End of stream
            }
            return fmt.Errorf("JSON decode error: %w", err)
        }
        
        // Process the item (memory can be GC'd after this)
        if err := callback(item); err != nil {
            return fmt.Errorf("callback error: %w", err)
        }
        
        processedCount++
    }
    
    return nil
}
```

### 2. Chunked Response Writing
Manages memory usage when writing large responses:

```mermaid
graph TB
    subgraph "Chunked Writing Strategy"
        Data[Large Data]
        Threshold{Size > Threshold?}
        DirectWrite[Direct Write to Output]
        BufferWrite[Buffer Small Writes]
        FlushBuffer[Flush Buffer When Full]
        MemCheck[Memory Usage Check]
        
        Data --> Threshold
        Threshold -->|Yes| DirectWrite
        Threshold -->|No| BufferWrite
        BufferWrite --> FlushBuffer
        DirectWrite --> MemCheck
        FlushBuffer --> MemCheck
    end
```

## OpenAPI Spec Optimization

### 1. Memory-Efficient Spec Loading
**Location**: `pkg/memory/openapi.go`

Optimizes OpenAPI specification loading and processing:

```mermaid
graph TB
    subgraph "Spec Optimization Flow"
        RawSpec[Raw OpenAPI Spec]
        SizeCheck[Size Validation]
        StreamLoad[Streaming Load]
        Optimize[Memory Optimization]
        Cache[Optimized Cache]
        
        RawSpec --> SizeCheck
        SizeCheck --> StreamLoad
        StreamLoad --> Optimize
        Optimize --> Cache
        
        subgraph "Optimization Techniques"
            RemoveExamples[Remove Examples]
            CompressSchemas[Compress Schemas]
            MinimalSerialization[Minimal Serialization]
        end
        
        Optimize --> RemoveExamples
        Optimize --> CompressSchemas
        Optimize --> MinimalSerialization
    end
```

#### Spec Memory Optimization
```go
func (mesl *MemoryEfficientSpecLoader) OptimizeSpec(spec *openapi3.T) error {
    // Remove examples from schema to save memory
    if spec.Components != nil && spec.Components.Schemas != nil {
        for _, schemaRef := range spec.Components.Schemas {
            if schemaRef.Value != nil {
                mesl.optimizeSchema(schemaRef.Value)
            }
        }
    }
    
    // Optimize paths by removing examples
    if spec.Paths != nil {
        for _, pathItem := range spec.Paths {
            if pathItem != nil {
                mesl.optimizePathItem(pathItem)
            }
        }
    }
    
    return nil
}

func (mesl *MemoryEfficientSpecLoader) optimizeSchema(schema *openapi3.Schema) {
    // Remove examples to save memory
    schema.Example = nil
    
    // Recursively optimize nested schemas
    if schema.Properties != nil {
        for _, propRef := range schema.Properties {
            if propRef.Value != nil {
                mesl.optimizeSchema(propRef.Value)
            }
        }
    }
}
```

### 2. Spec Compression for Storage
Compresses specifications for efficient storage:

```mermaid
graph LR
    subgraph "Spec Compression Pipeline"
        FullSpec[Full OpenAPI Spec]
        Essential[Extract Essential Parts]
        Minimal[Create Minimal Spec]
        Serialize[JSON Serialization]
        Compressed[Compressed Storage]
        
        FullSpec --> Essential
        Essential --> Minimal
        Minimal --> Serialize
        Serialize --> Compressed
    end
    
    subgraph "Memory Savings"
        Examples[Remove Examples: -40%]
        Descriptions[Minimize Descriptions: -20%]
        Metadata[Remove Metadata: -15%]
        Total[Total Savings: ~75%]
    end
```

## Batch Processing Architecture

### Large Dataset Handling
Processes large datasets in memory-efficient batches:

```mermaid
graph TB
    subgraph "Batch Processing Flow"
        Dataset[Large Dataset]
        Splitter[Batch Splitter]
        
        subgraph "Processing Pipeline"
            Batch1[Batch 1]
            Batch2[Batch 2]
            BatchN[Batch N]
            
            Process1[Process Batch 1]
            Process2[Process Batch 2]
            ProcessN[Process Batch N]
            
            Batch1 --> Process1
            Batch2 --> Process2
            BatchN --> ProcessN
        end
        
        Collector[Result Collector]
        MemMonitor[Memory Monitor]
        
        Dataset --> Splitter
        Splitter --> Batch1
        Splitter --> Batch2
        Splitter --> BatchN
        
        Process1 --> Collector
        Process2 --> Collector
        ProcessN --> Collector
        
        MemMonitor --> Process1
        MemMonitor --> Process2
        MemMonitor --> ProcessN
    end
```

#### Batch Processing Implementation
```go
func (ldh *LargeDataHandler) ProcessInBatches(ctx context.Context, data []interface{}, batchSize int, processor func([]interface{}) error) error {
    for i := 0; i < len(data); i += batchSize {
        // Check context cancellation
        select {
        case <-ctx.Done():
            return ctx.Err()
        default:
        }
        
        // Check memory usage
        if !ldh.processor.CheckMemory() {
            return fmt.Errorf("memory usage exceeded limits")
        }
        
        // Create batch
        end := i + batchSize
        if end > len(data) {
            end = len(data)
        }
        
        batch := data[i:end]
        
        // Process batch (memory can be GC'd after this)
        if err := processor(batch); err != nil {
            return fmt.Errorf("batch processing error at index %d: %w", i, err)
        }
    }
    
    return nil
}
```

## Performance Metrics

### Memory Usage Patterns

```mermaid
graph LR
    subgraph "Memory Usage Over Time"
        Time[Time â†’]
        
        subgraph "Without Optimization"
            Linear[Linear Growth]
            Peak[Memory Peaks]
            OOM[Out of Memory]
        end
        
        subgraph "With Optimization"
            Stable[Stable Usage]
            Controlled[Controlled Peaks]
            Efficient[Memory Efficient]
        end
    end
    
    Linear -.-> Stable
    Peak -.-> Controlled
    OOM -.-> Efficient
```

### Optimization Impact

| Operation | Before Optimization | After Optimization | Improvement |
|-----------|-------------------|-------------------|-------------|
| **Large Spec Loading** | 500MB+ memory | 50MB memory | 90% reduction |
| **JSON Processing** | Linear growth | Constant memory | Prevents OOM |
| **Buffer Operations** | New allocations | Pooled buffers | 80% fewer allocations |
| **Concurrent Requests** | Memory multiplication | Shared pools | 60% memory savings |

## Configuration Examples

### Memory Limiter Configuration
```go
// Configure memory limiter for production
memoryLimiter := memory.NewMemoryLimiter(512) // 512MB limit

// Configure stream processor with chunking
streamProcessor := memory.NewStreamProcessor(256, 8192) // 256MB limit, 8KB chunks

// Configure spec loader with size limits
specLoader := memory.NewMemoryEfficientSpecLoader(512, 100) // 512MB total, 100MB per spec
```

### Buffer Pool Configuration
```go
// Configure pools based on expected load
bytePool := memory.NewBytePool(8192)    // 8KB initial size
bufferPool := memory.NewBufferPool()     // Auto-sizing buffers

// Use pools in processing
func processLargeData() {
    buffer := bytePool.Get()
    defer bytePool.Put(buffer)
    
    // Process data with pooled buffer
}
```

## Monitoring and Observability

### Memory Metrics Collection
```mermaid
graph TB
    subgraph "Memory Monitoring"
        Collector[Metrics Collector]
        
        subgraph "Metrics"
            AllocMem[Allocated Memory]
            SysMem[System Memory]
            GCCount[GC Count]
            PoolStats[Pool Statistics]
        end
        
        subgraph "Alerts"
            HighMem[High Memory Usage]
            FrequentGC[Frequent GC]
            PoolExhaustion[Pool Exhaustion]
        end
        
        Collector --> AllocMem
        Collector --> SysMem
        Collector --> GCCount
        Collector --> PoolStats
        
        AllocMem --> HighMem
        GCCount --> FrequentGC
        PoolStats --> PoolExhaustion
    end
```

### Integration with Application Monitoring
- Memory usage tracking per request
- Pool efficiency metrics
- GC performance monitoring
- Alert thresholds for memory limits

---

*This memory optimization architecture ensures the system can handle large-scale operations while maintaining predictable memory usage and preventing out-of-memory conditions.*